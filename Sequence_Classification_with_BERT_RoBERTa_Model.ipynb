{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1kQXfvNV4FcyIi3KGUQVRxC-Zz7ouHF0K",
      "authorship_tag": "ABX9TyNsrZSH6DsHEq/Sqhg3Kxlg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chitlchow/ai-text-detection-bert/blob/main/Sequence_Classification_with_BERT_RoBERTa_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KCbOEiyN9Am",
        "outputId": "dcccb63f-b1ca-491a-9fad-758a8ac395fe",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.7 torchmetrics-1.4.2\n"
          ]
        }
      ],
      "source": [
        "# @title Modules Import\n",
        "!pip install datasets transformers\n",
        "!pip install kaggle torchmetrics\n",
        "from datasets import load_dataset, load_dataset_builder\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertTokenizer, RobertaTokenizer, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import string\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Choice"
      ],
      "metadata": {
        "id": "hRAfcWruvNNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, RobertaModel\n",
        "from torch import nn\n",
        "\n",
        "model_name = 'bert-base-uncased' # @param ['bert-base-case', 'bert-base-uncased', 'roberta-base'] {'type': 'string'}\n",
        "\n",
        "class TransformerTextClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    TransformerTextClassifier\n",
        "    :attr model_name: str, name of the model to use\n",
        "    :attr transformer: nn.Module, transformer model for processing text tokens into vectors\n",
        "    :attr classifier: nn.Module, classifier module for processing vector representations of text into final outputs\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.transformer = BertModel.from_pretrained(model_name) # @param ['BertModel.from_pretrained(model_name)', 'RobertaModel.from_pretrained(model_name)']  {'type': 'raw'}\n",
        "        # Classifer module\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Taking the transformer output for classification\n",
        "        x = self.transformer(input_ids, attention_mask).pooler_output\n",
        "        output = self.classifier(x)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "CX-uUvXQXZ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation and Tokenizer Settings"
      ],
      "metadata": {
        "id": "3joxqMIKZL1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, RobertaTokenizer\n",
        "ds_name = 'aadityaubhat/GPT-wiki-intro' # @param {'type':'string'}\n",
        "import random\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    TextDataset\n",
        "\n",
        "    Setup dataset for training and testing using torch Dataset Class and ready for use in dataloader\n",
        "    :param::ds_name: str, name of the dataset to use\n",
        "    :param::model_name: str, name of the model to use\n",
        "    :attr dataset: Dataset, dataset to use\n",
        "    :attr tokenizer: nn.Module, tokenizer to use\n",
        "    \"\"\"\n",
        "    def __init__(self, ds_name:\n",
        "                 str=ds_name, model_name: str=model_name) -> None:\n",
        "        self.dataset = load_dataset(ds_name,split='train')\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name) # @param ['BertTokenizer.from_pretrained(model_name)', 'RobertaTokenizer.from_pretrained(model_name)'] {'type':'raw'}\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the data in JSON format\n",
        "        json_data = self.dataset[idx]\n",
        "\n",
        "        # Randomly choosing a source\n",
        "        source = random.choice(['wiki_intro', 'generated_intro'])\n",
        "        # Assigning the text data to be drawn\n",
        "        text = json_data[source]\n",
        "        # create label depending on the source\n",
        "        label = 0 if source == 'wiki_intro' else 1\n",
        "        # Create tokens for model inference\n",
        "        tokens = self.tokenizer(\n",
        "            text, max_length=512, padding='max_length', truncation=True, return_tensors='pt'\n",
        "            )\n",
        "        input_ids = tokens['input_ids']\n",
        "        attention_mask = tokens['attention_mask']\n",
        "\n",
        "        # Return token, label\n",
        "        return (input_ids, attention_mask), label\n",
        "\n",
        "train_dataset = TextDataset()"
      ],
      "metadata": {
        "id": "7t1NOyLd1UCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "7BxN-KtOuytV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Parameters\n",
        "from torchmetrics import Accuracy, Precision, Recall, F1Score\n",
        "from torch import optim\n",
        "\n",
        "# Model to CUDA\n",
        "model = TransformerTextClassifier()\n",
        "\n",
        "# Initialize bias and weight\n",
        "weight_init_method = 'zero' # @param ['zero', 'normal', 'None'] {'type':'string'}\n",
        "if weight_init_method == 'zero':\n",
        "    nn.init.zeros_(model.transformer.pooler.dense.bias)\n",
        "    nn.init.zeros_(model.transformer.pooler.dense.weight)\n",
        "elif weight_init_method == 'normal':\n",
        "    nn.init.normal_(model.transformer.pooler.dense.bias)\n",
        "    nn.init.normal_(model.transformer.pooler.dense.weight)\n",
        "else:\n",
        "    pass\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 3 # @param {\"type\":\"number\"}\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW( # @param [\"optim.AdamW(\"] {\"type\":\"raw\"}\n",
        "    model.parameters(),\n",
        "lr = 2e-5 # @param {\"type\":\"number\"}\n",
        ")\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32 # @param ['4', '8', '12', '16', '32']{'type': 'raw'}\n",
        "    ,shuffle=True\n",
        ")\n",
        "print(f'Training Steps: {len(train_dataloader)}')\n",
        "\n",
        "# Setup metrics\n",
        "acc = Accuracy(task='multiclass', num_classes=2, average='micro').to(device)\n",
        "precision = Precision(task='multiclass', num_classes=2, average='micro').to(device)\n",
        "f1 = F1Score(task='multiclass', num_classes=2, average='micro').to(device)\n",
        "recall = Recall(task='multiclass', num_classes=2, average='micro').to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Reseting running loss\n",
        "    running_loss = 0\n",
        "    # Model to training mode\n",
        "    model.train()\n",
        "    for (input_ids, attention_masks), labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        # Produce output from model\n",
        "        input_ids, attention_masks = input_ids.squeeze().to(device), attention_masks.squeeze().to(device)\n",
        "        output = model(input_ids, attention_masks)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Compute Metrics\n",
        "        loss = criterion(output, labels)\n",
        "        acc(output, labels)\n",
        "        precision(output, labels)\n",
        "        f1(output, labels)\n",
        "        recall(output, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Back propagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Compute Overall all metrics\n",
        "    epoch_acc = acc.compute()\n",
        "    epoch_precision = precision.compute()\n",
        "    epoch_f1 = f1.compute()\n",
        "    epoch_recall = recall.compute()\n",
        "    print(\n",
        "        f'Epoch: {epoch+1}, Loss: {running_loss:.4f}, Accuracy: {epoch_acc:.4f}, Precision: {epoch_precision:.4f}, F1: {epoch_f1:.4f}, Recall: {epoch_recall:.4f}'\n",
        "        )\n",
        "\n",
        "    # Reset all metrics for monitoring\n",
        "    acc.reset()\n",
        "    precision.reset()\n",
        "    f1.reset()\n",
        "    recall.reset()\n",
        "\n",
        "print('Training Complete')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCeKecp4Cjl2",
        "outputId": "f2bd202e-208c-4e6d-a235-2e7c43b4b2a6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Steps: 4688\n",
            "Epoch: 1, Loss: 1033.4609, Accuracy: 0.6955, Precision: 0.6955, F1: 0.6955, Recall: 0.6955\n",
            "Epoch: 2, Loss: 24.1580, Accuracy: 0.9986, Precision: 0.9986, F1: 0.9986, Recall: 0.9986\n",
            "Epoch: 3, Loss: 13.6294, Accuracy: 0.9992, Precision: 0.9992, F1: 0.9992, Recall: 0.9992\n"
          ]
        }
      ]
    }
  ]
}